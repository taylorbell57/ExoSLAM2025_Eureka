{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f966d9-d9c3-4432-a523-9841e1873d07",
   "metadata": {},
   "source": [
    "<img src='https://eurekadocs.readthedocs.io/en/latest/_images/Eureka_logo.png' alt=\"eureka_logo\" width=\"400px\"/><img src='https://exoclimes.org/img/exoslam-bg.png' alt=\"ariel_france_logo\" width=\"220px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47e70f-1f9a-4855-9b01-5b41ebf2abaa",
   "metadata": {},
   "source": [
    "# ExoSLAM 2025\n",
    "## Bonus Tutorial - Eureka Fitting of MIRI/LRS Data\n",
    "\n",
    "**Authors**: Taylor James Bell (ESA/AURA for STScI)<br>\n",
    "**Last Updated**: June 25, 2025<br>\n",
    "**jwst Pipeline Version**: 1.18.0 (Build 11.3)<br>\n",
    "**Eureka! Pipeline Version**: [exoslam2025](https://github.com/kevin218/Eureka/tree/exoslam2025) (based on Eureka! version 1.2.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f501a5-b003-48a4-ad5c-078788f7d680",
   "metadata": {},
   "source": [
    "**Purpose**:<br/>\n",
    "\n",
    "The objective of this notebook is to gain familiarity with how space-telescope spectroscopy is \"analyzed\", which is a term often used to describe the process of going from measurements of the system's brightness as a function of time (called spectroscopic lightcurves) to planetary transmission or emission spectra.\n",
    "\n",
    "**Data**:<br/>\n",
    "This notebook is set up to use an example dataset is from [Program ID](https://www.stsci.edu/jwst/science-execution/program-information) 1366 (PI: Batalha, Natalie) which is the JWST Transiting Exoplanet Community ERS program. In particular, we will use the MIRI/LRS full-orbit phase curve of the hot Jupiter WASP-43b, which was first published in [Bell et al. (2024)](https://ui.adsabs.harvard.edu/abs/2024NatAs...8..879B/abstract). These observations continuously monitored the WASP-43 system for 26.5 hours and contain two eclipses of WASP-43b, one transit of WASP-43b, and the orbital phase variations caused by changes in the regions of WASP-43b's atmosphere that were pointed toward JWST throughout the planet's orbit.\n",
    "\n",
    "For our purposes, we are only going to work on the first 6 segments of the first exposure to keep things fairly speedy while still getting a realistic sense of what it is like to reduce MIRI/LRS data.\n",
    "\n",
    "**JWST pipeline version and CRDS context**:<br/>\n",
    "This notebook was written for the calibration pipeline version given above and uses the context associated with this version of the JWST Calibration Pipeline. Information about this and other contexts can be found in the JWST Calibration Reference Data System (CRDS) [server]((https://jwst-crds.stsci.edu/)). If you use different pipeline\n",
    "versions, please refer to the table [here](https://jwst-crds.stsci.edu/display_build_contexts/) to determine what context to use. To learn more about the differences for the pipeline, read the relevant [documentation](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline/jwst-operations-pipeline-build-information)\n",
    "\n",
    "**Eureka! pipeline version**:<br/>\n",
    "Eureka is a pipeline developed to reduce and analyze HST and JWST data ([Bell et al. 2023](https://joss.theoj.org/papers/10.21105/joss.04503.pdf)). \n",
    "The code is available on [Github](https://github.com/kevin218), and there is also detailed [online documentation](eurekadocs.readthedocs.io).\n",
    "This notebook was written for the particular Eureka! version specified above. If you are not using that particular version, you must ensure that all of your ECF and EPF settings are adjusted as necessary (looking at the demos/JWST folder for the Eureka! version you have installed is a good way to check whether any parameters changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b3e45-21bd-4e26-a61b-6c5fb3f5342b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [0. Configuration](#0.-Configuration)\n",
    "* [5. Sampling white lightcurve posterior](#5.-Sampling-white-lightcurve-posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756661c9-1c7e-4cac-acd4-d96c107274aa",
   "metadata": {
    "id": "Z6esX4RP-G7G"
   },
   "source": [
    "---\n",
    "## 0. Configuration\n",
    "\n",
    "The first step is to setup the notebook and environment.\n",
    "\n",
    "We'll first import Eureka! along with some other useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a784d0",
   "metadata": {
    "id": "55a784d0"
   },
   "outputs": [],
   "source": [
    "import batman\n",
    "import eureka\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BaLqbuvPsXjg",
   "metadata": {
    "id": "BaLqbuvPsXjg"
   },
   "source": [
    "Next, we will re-use the eventlabel from Notebook #1. The eventlabel is a short, meaningful label (without spaces) that describes the data you're currently working on. For simplicity, simply set `eventlabel = 'miri_exoslam'`. This same event label should be used throughout all stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd18c07",
   "metadata": {
    "id": "4bd18c07"
   },
   "outputs": [],
   "source": [
    "eventlabel = ''  # <--- Please update this as described above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf62391-11b2-47c8-b72f-635d9e22379f",
   "metadata": {},
   "source": [
    "Please adjust the following variable to specify specify where you downloaded the data in the Setup.ipynb notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9b69b-da55-4178-9597-b385108188ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_data_folder_on_your_machine = '../data/'  # <--- Please update this variable if needed to match your data location\n",
    "path_to_data_folder_on_your_machine = os.path.expanduser(path_to_data_folder_on_your_machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seVhRGioPDrg",
   "metadata": {
    "editable": true,
    "id": "seVhRGioPDrg",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "### An introduction to Bayesian statistics\n",
    "\n",
    "*The text discussing Bayesian statistics throughout this notebook is adapted from:*\n",
    "<blockquote>Bell, Taylor James. \"Characterizing Ultra-Hot Jupiters through Theoretical Modelling and Precise Observations.\" Doctoral Thesis, McGill University, 2021.</blockquote>\n",
    "\n",
    "<br/>\n",
    "\n",
    "**This introduction to Bayesian statistics can be skipped if you are already familiar, and you can resume at \"Optimizing a fit to the white lightcurve\".**\n",
    "\n",
    "<br/>\n",
    "\n",
    "This workshop will make heavy use of Bayesian statistics which is a fundamental aspect of most aspects of modern science and data analysis. According to Bayes' theorem, the probability, $P$, of a hypothesis, $H$, given a set of observations, $\\mathbf{X}$, and a collection of prior knowledge, $I$, is given by\n",
    "\\begin{equation*}\n",
    "    P(H|\\mathbf{X},I) = \\frac{P(\\mathbf{X}|H,I) P(H|I)}{P(\\mathbf{X}|I)},\n",
    "\\end{equation*}\n",
    "where $|$ reads as \"given\" (that all terms to the right are assumed true), and the comma reads as \"and\". The power of Bayes' theorem is that it allows us to compute the *posterior probability*, $P(H|\\mathbf{X},I)$, using the much more easily calculable *likelihood function*, $P(\\mathbf{X}|H,I)$, which is the probability that we would have observed the data $\\mathbf{X}$ if the hypothesis and prior knowledge were correct. The $P(H|I)$ term is the *prior probability* and summarizes how our prior knowledge affects our hypothesis before having measured the data $\\mathbf{X}$. Finally, the $P(\\mathbf{X}|I)$ term is the *evidence* or *marginal likelihood* and is often omitted when fitting a model to data as it is only a normalization term and does not depend on the hypothesis.\n",
    "\n",
    "When fitting a set of observations, a hypothesis typically consists of a function describing the model which depends on a collection of parameters, $\\theta$, and hyperparameters, $\\alpha$. Bayes' theorem can then be re-written as\n",
    "\\begin{equation*}\n",
    "    P(\\theta|\\mathbf{X},\\alpha) \\propto P(\\mathbf{X}|\\theta,\\alpha) P(\\theta|\\alpha).\n",
    "\\end{equation*}\n",
    "Fitting the observations usually starts by freezing the set of hyperparameters and then evaluating the posterior probability by comparing different model predictions to the observed data. Fitting the observations then requires determining the values of $\\theta$ that maximize the posterior probability (called the Maximum A Posteriori estimate or MAP estimate), while determining the uncertainty on the model parameters involves determining the range of values of $\\theta$ that provide an adequately good fit to the observations (called the confidence interval).\n",
    "\n",
    "In principle, one could simultaneously estimate the optimal value of $\\theta$ and its confidence interval from the posterior probability density function (PDF) by calculating the posterior probability for all values of $\\theta$; this is called a grid search. While this technique may be feasible for discrete parameters or low dimensional problems, performing a grid search when the vector $\\theta$ contains tens or thousands of continuous variables becomes immensely challenging and computationally inefficient. Instead, various algorithms can be used to compute the MAP estimate and the confidence interval, some of which we will make use of in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BpxySerl9dBb",
   "metadata": {
    "editable": true,
    "id": "BpxySerl9dBb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "## 1. Sampling white lightcurve posterior\n",
    "\n",
    "A sampling algorithm like the Markov Chain Monte-Carlo (MCMC) method is typically used to estimate parameter best-fit values and uncertainties. Monte-Carlo (MC) methods in general involve randomly sampling values of $\\theta$, while MCMCs are a specific variant where samples are randomly drawn based on the knowledge of the posterior probability from the previously drawn value; the draws of MCMC are typically called steps taken by a walker, and a collection of many walker steps are called a chain.\n",
    "\n",
    "One increasingly popular sampling algorithm is called [Nested Sampling](https://en.wikipedia.org/wiki/Nested_sampling_algorithm) which does not necessarily use an MCMC method. Nested sampling can be difficult to explain intuitively, but one of the nice advantages to nested sampling is that it can be used for model comparison. More relevant to our purposes here, the [dynesty](https://dynesty.readthedocs.io/en/stable/) dynamic nested sampling algorithm is able to quickly sample our posterior and has a convenient stopping condition which ensures the sampler has converged so that we get reliable uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mhJwRuf9uip",
   "metadata": {
    "editable": true,
    "id": "9mhJwRuf9uip",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "⚠️ Note that Stage 5 has TWO control files, one `.ecf`and one `.epf`⚠️\n",
    "\n",
    "### 1.1 Setting the Stage 5 \"Eureka! Control File\" (ECF)\n",
    "\n",
    "**This determines what will happen during Stage 5**\n",
    "\n",
    "To begin, please first copy below the contents of the ECF template for MIRI/LRS from the `S5_template.ecf` file in the ECF demos folder on [GitHub](https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST).\n",
    "\n",
    "The most important parameters and their recommended settings are described below, but more context can be found on the [Eureka! documentation website](https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5).\n",
    "\n",
    "1. Set `ncpu` to the number of CPU threads you want to use. If set to `1` no multiprocessing will be done, and this parameter can be increased to ~2x your CPU core count for faster runs.\n",
    "3. Set `verbose` to `True` so you get lots of useful information printed out.\n",
    "4. Set `fit_method` to `[dynesty]` to instead use the dynesty sampler.\n",
    "5. Set `run_myfuncs` to `[batman_ecl, polynomial, expramp, ypos, ywidth]`. Each element in this list is a function that will be fitted to the data — we'll setup more aspects of each function below when we setup the \"Eureka! Parameter File\". The `batman_ecl` function uses the [batman](http://lkreidberg.github.io/batman/docs/html/index.html) package to model an eclipse function which describes how the astrophysical flux decreases when we go from seeing the combined light from the star and planet to just the light from the star when the planet is behind the star. The `polynomial` function allows us to fit for systematic trends in time (like a linear slope) as well as fitting for the overall baseline flux level; you must always use a `polynomial` model when fitting observations to at least model the baseline flux level. The `expramp` function allows us to model the detector systematic ramp in MIRI data caused by some combination of detector persistence/settling. The `ypos` and `ywidth` functions model changes in the flux caused by changes in the position and PSF width of the spectrum on the detector in the spatial direction; we computed these during Stage 3 in Notebook #1. JWST is quite stable over time, so these functions just assume that the flux is linearly correlated with the spatial position and PSF width. Not all observations require that you use the `ypos` and `ywidth` functions, but they're often useful for MIRI observations.\n",
    "6. Set `fit_par` to `./S5_fit_par_template.epf`. This tells Eureka! where you have specified the priors and initial guesses for the parameters the control the functions listed above.\n",
    "7. Set `manual_clip` to `[[None,975]]` to remove **a lot** of the initial integrations which are worst affected by the initial exponential ramp. You **do not** always remove this many integrations, but we will for this demonstration for multiple reasons.\n",
    "8. Leave all parameters under the \"Limb darkening controls\" heading at their default values of `None` since we're fitting an eclipse observation and stellar limb darkening has no impact on eclipse observations.\n",
    "9. Add the following dynesty-related settings\n",
    "  1. Set `run_nlive` to `121`. This sets the number of \"live\" points that dynesty will use (for more details see [here](https://dynesty.readthedocs.io/en/stable/faq.html#live-point-questions)). For reliable results, this must be greater than `ndim * (ndim + 1) // 2`, where `ndim` is the number of fitted parameters and `//` indicates integer division. In general, the larger this number the more robust your results will be and the longer they will take to run. Setting `run_nlive` to a value of `'min'` will make sure the minimum number of live points is respected and is often sufficient for initial analyses, while more in-depth analyses might benefit from using a larger number of live points (like our value of `121`).\n",
    "  2. Set `run_bound` to `'multi'`. This sets the type of bounds used by dynesty to \"multiple ellipsoid decomposition\". For more information, see [here](https://dynesty.readthedocs.io/en/stable/faq.html#bounding-questions). A value of `'multi'` should work for most problems.\n",
    "  3. Set `run_sample` to `'rwalk'`. This is often the optimal sampling method but that sometimes depends on the number of fitted parameters, and `auto` can sometimes also be a safe bet for most problems. For more information, see [here](https://dynesty.readthedocs.io/en/stable/faq.html#sampling-questions).\n",
    "  4. Finally, set `run_tol` to `0.01`. This determines the stopping condition of the sampler. Smaller values will result in more precise posteriors at the cost of increased runtime, while larger values can result in poorly converged garbage. In general, a value of `0.1` should work for initial investigations, while `0.01` might work better when producing your \"final\" results.\n",
    "10. Set `isplots_S5` to `5` so that you see lots of useful plots that can help you debug your fits.\n",
    "11. Set `nbin_plot` to `None` since we're not fitting that many integrations and we don't really need to temporally bin the data when plotting to be able to see our astrophysical signal.\n",
    "12. Set `hide_plots` to `False` so that all your figures appear within the notebook (you can set this to `True` if you're running the code from the terminal and want to avoid an excessive number of pop-ups appearing).\n",
    "13. Set `topdir` to `{path_to_data_folder_on_your_machine}` which will use the folder you specified above.\n",
    "14. Since we're fitting white lightcurves, we'll adjust the `inputdir` to `Stage4_white` and the `outputdir` to `Stage5_white`. To choose a specific run, use the syntax `Stage4_white/S4_YYYY-MM-DD_miri_lrs_runN` where YYYY is the year (e.g., `2025`), MM is the month (e.g., `04`), DD is the day (e.g., `18`), and N is the particular run number that you want to use (e.g., `1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dGSLSFSv9Qmd",
   "metadata": {
    "id": "dGSLSFSv9Qmd"
   },
   "outputs": [],
   "source": [
    "s5_ecf_contents = f\"\"\"\n",
    "\n",
    "# Fill this f-string text block with the contents of the S5 ECF template\n",
    "# from https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST\n",
    "# and then adjust the values as described above.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(f'S5_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s5_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AObs5Cdm0gzA",
   "metadata": {
    "id": "AObs5Cdm0gzA"
   },
   "source": [
    "### 1.2 Setting the Stage 5 \"Eureka! Parameter File\" (EPF)\n",
    "\n",
    "**This determines the priors and starting guesses for your fitted models**\n",
    "\n",
    "To begin, please first copy below the contents of the EPF template from the `S5_fit_par_template.epf` file in the ECF demos folder on [GitHub](https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST).\n",
    "\n",
    "Here is where we dive further into specifying details about the models that we want to fit to our data. We'll discuss the ones relevant to this fit below, but more details can be found on the [Eureka documentation webpage](https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-5-fit-parameters).\n",
    "\n",
    "Each row has 6 columns:\n",
    "> `Name Value Free? PriorPar1 PriorPar2 PriorType`\n",
    "\n",
    "`Name` specifies the name of a fitted variable. These have to be from a very specific list of options so that Eureka! knows what the variable is supposed to mean. For example `rp` is the planet-to-star radius ratio (so `rp` squared would be equal to the transit depth) and `fp` is the eclipse depth.\n",
    "\n",
    "`Value` specifies the starting guess for the variable. This column is used for all optimization or sampling algorithms except dynesty which does not take a starting guess. In general, starting guesses do not need to be exceptionally good, and as long as they're in the vague ballpark then the fitting or sampling algorithm should work fine. For fixed parameters (see the description of `Free?` below), this column sets the value of the parameter.\n",
    "\n",
    "`Free?` specifies whether the parameter should be freely fitted for each wavelength (`'free'`), should be set to a fixed value and not changed throughout the fit (`'fixed'`), is an auxilliary variable that also should not be fit (`'independent'`). Other more advanced options also exist, but we won't cover them here. For fixed and independent variables, the remaining three columns are not used.\n",
    "\n",
    "**The rightmost column**, `PriorType`, determines what type of prior function you will use to constrain your fitted parameters. The three options are Normal (`N`; also known as a Gaussian prior), Uniform (`U`), and Log-Uniform (`LU`). Some sampling algorithms work faster or more efficiently with Normal priors, but sometimes you must use Uniform priors to avoid unphysical parameter settings. Normal priors are typically best for orbital parameters for which there are published best-fit values and uncertainties.\n",
    "\n",
    "The meanings of the `PriorPar1` and `PriorPar2` columns are dependent on the setting of the `PriorType` column (which is the last column). If `PriorType` is N, then `PriorPar1` is the mean of the Gaussian prior and `PriorPar2` is the standard deviation. If `PriorType` is U, then `PriorPar1` is the lower limit of the parameter and `PriorPar2` is the upper limit of the parameter. If `PriorType` is LU, then `PriorPar1` is the lower limit of the log of the parameter, and `PriorPar2` is the upper limit of the log of the parameter.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "__Our astrophysical models are controlled by the following parameters__:\n",
    "\n",
    "- For `per` (planetary orbital period), `t0` (time of conjunction; also called linear ephemeris or transit midpoint), `inc` (planetary orbital inclination), `a` (planetary orbital semi-major axis), `ecc` (planetary orbital eccentricity), and `w` (planetary orbital argument of periastron) we will make use of previously published research. In general, one can navigate to the [NASA Exoplanet Archive's page](https://exoplanetarchive.ipac.caltech.edu/overview/WASP-43b) about our target WASP-43b and click on \"WASP-43 b Planetary Parameters\". However, since these MIRI/LRS data were already published and give more accurate orbital parameters, we will use the values published by [Hammond et al. 2024](https://ui.adsabs.harvard.edu/abs/2024AJ....168....4H/abstract) (Table 1, \"Eclipse Map Fit\") as priors. Where there are two values for the uncertainty, just use the larger of the two. Also, while the published `t0` is in units of BJD_TDB, ⚠️ JWST timestamps are in units of B**M**JD_TDB ⚠️, where the M means \"modified\" and BMJD_TDB=BJD_TDB-2400000.5; this removes the first two digits that won't change in our lifetime and changes the time so that a new BMJD_TDB day starts at midnight (while new BJD_TDB days inconventiently start at noon). In general we would need to subtract that 2400000.5 from the published `t0` value when entering it into your EPF, but [Hammond et al. 2024](https://ui.adsabs.harvard.edu/abs/2024AJ....168....4H/abstract) published the `t0` in units of BMJD_TDB already. [Hammond et al. 2024](https://arxiv.org/abs/2404.16488) did not re-fit the orbital period, `per`, and instead just fixed the orbital period to `0.813474056` days from [Kokori et al. 2023](https://ui.adsabs.harvard.edu/abs/2023ApJS..265....4K/abstract); we will do the same. And since the planet is consistent with a zero-eccentricity orbit, let's set `ecc` to be `'fixed'` to `0` and arbitrarily set `w` to be `'fixed'` to `90` (the value of `w` doesn't matter if the eccentricity is zero). Additionally, with only a single eclipse we will likely be unable to significantly improve the constraints on any of these orbital parameters with the exception of the mid-eclipse time, so we will set all these orbital parameters except `t0` to be `'fixed'` to their published values.\n",
    "- In summary, you should have:\n",
    "> `per 0.813474056 'fixed'`<br/>\n",
    "> `t0 55934.292283 'free' 55934.292283 0.000011 N`<br/>\n",
    "> `inc 82.106 'fixed'`<br/>\n",
    "> `a 4.859 'fixed'`<br/>\n",
    "> `ecc 0 'fixed'`<br/>\n",
    "> `w 90 'fixed'`<br/>\n",
    "\n",
    "- For `Rs` (the stellar radius in units of Solar radii), we can again use the [NASA Exoplanet Archive's page](https://exoplanetarchive.ipac.caltech.edu/overview/WASP-43b) about WASP-43, this time instead clicking on the \"WASP-43 Stellar Parameters\" section. This parameter is only used to account for the finite speed of light, so simply fixing it to the published value from [Bonomo et al. 2017](https://exoplanetarchive.ipac.caltech.edu/overview/WASP-43) will work:\n",
    "> `Rs 0.667 'fixed'`\n",
    "\n",
    "- For `rp` (planet-to-star radius ratio), we'll use the radius fitted from a later part of these data where the planet transited. Please set `rp` to be fixed to a value of 0.15839 based on the results of [Hammond et al. 2024](https://ui.adsabs.harvard.edu/abs/2024AJ....168....4H/abstract) (Table 1, \"Eclipse Map Fit\"). Without this knowledge, one could also compute the planet-to-star radius ratio from the planet and star radii published by Bonomo et al. 2017, although care would need to be taken to ensure units were properly accounted for. In summary, set:\n",
    "> `rp 0.15839 'fixed'`\n",
    "\n",
    "- Set time_offset to the following to ignore the parameter:\n",
    "> `time_offset 0 'independent'`.\n",
    "\n",
    "- For `fp` (eclipse depth) we will have to create a \"minimally informative prior\". With this prior, we seek to constrain the model to the very rough area that we expect for the parameters while minimally influencing the final results with our semi-arbitrary decision. For `fp`, a visual inspection of the lightcurve produced in the last notebook suggests that the eclipse depth is around 6000 ppm, so a reasonable, \"minimally informative\" prior could be $6000 \\pm 5000$ ppm which would be specified as:\n",
    "  > `fp 6000e-6 'free' 6000e-6 5000e-6 N`\n",
    "\n",
    "  where `6000e-6` is a more conventient way of specifying 6000 ppm than carefully counting decimal places and specifying `0.006` (although both options are possible).\n",
    "\n",
    "__Our systematic models are controlled by__:\n",
    "\n",
    "  - `c0` (the baseline flux level) and `c1` (which fits for a linear slope as a function of time). These two parameters control the `polynomial` function we specified in the ECF settings above. To make things easier, Eureka! normalizes the lightcurves by the median flux, so the value of `c0` should be approximately 1, but more precisely should be set to the value of the normalized flux during the middle of the eclipse observation; by eye this appears to be around 0.996. The value of `c1` is in the units of change in `c0` per day, and is typically very small (the absolute value of `c1` is typically at or below 0.001. Reasonable, minimally informative priors could then be:\n",
    "  > `c0 0.996 'free' 0.996 0.01 N`<br/>\n",
    "  > `c1 0 'free' 0 0.01 N`\n",
    "  - `r0` (the exponential ramp amplitude) and `r1` (the exponential ramp timescale). These two parameters control the `expramp` function we specified in the ECF settings above. The value of `r0` controls the amplitude of the exponential ramp and should be close to 0 given how many initial integrations we have trimmed. The value of `r1` is the exponential decay rate in units of days<sup>-1</sup>, and is typically in the range of 5-500. Reasonable, minimally informative priors could then be:\n",
    "  > `r0 0 'free' 0 0.01 N`<br/>\n",
    "  > `r1 200 'free' 5 500 U`\n",
    "  - `ypos` which assumes the observed flux varies linearly with the position on the detector. This can happen because of flux lost outside of our extraction aperture or between gaps between pixels. Values for `ypos` are typically well below 1, so a reasonable, minimally informative prior could then be:\n",
    "  > `ypos 0 'free' 0 0.1 N`\n",
    "  - and `ywidth` which assumes the observed flux varies linearly with the PSF width of the spectrum. This can happen because of flux lost outside of our extraction aperture or between gaps between pixels. Values for `ywidth` can sometimes reach as high as ~1, so a reasonable, minimally informative prior could then be:\n",
    "  > `ywidth 0 'free' 0 10 N`\n",
    "\n",
    "- We also need to setup `scatter_mult`. Eureka! estimates the expected level of noise from photon-limited statistics, but data is imperfect and we rarely ever reach the photon-limited noise level (because of background noise, read noise, and other systematic noise sources). If you fit data with underestimated error bars, then the resulting confidence intervals can be artificially small, so we want to inflate the error bars on our observations as is justified by the data. The simplest way to do this with Eureka! is the `scatter_mult` parameter which multiplies the error bars by a constant factor. A fitted value of 2 for `scatter_mult` would mean that the observed scatter in the data is twice as high as it would be in the photon-limited noise regime. MIRI data is typically pretty well behaved though, and a reasonable, minimally informative prior could be:\n",
    "> `scatter_mult 1.1 'free' 0.8 10 U`\n",
    "\n",
    "**Finally, delete or comment-out (with `#`) all other parameters not mentioned above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5uM_6A7W0gRJ",
   "metadata": {
    "id": "5uM_6A7W0gRJ"
   },
   "outputs": [],
   "source": [
    "s5_epf_contents = f\"\"\"\n",
    "\n",
    "# Fill this f-string text block with the contents of the S5 EPF template\n",
    "# from https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST\n",
    "# and then adjust the values as described above.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('S5_fit_par_template.epf', 'w') as f:\n",
    "    f.write(s5_epf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dmgakxE95of",
   "metadata": {
    "id": "9dmgakxE95of"
   },
   "source": [
    "### 1.3 Running Eureka!'s Stage 5\n",
    "\n",
    "The following cell will run Eureka!'s Stage 5 using the settings you defined above. Note that your ECF and EPF will be copied to your output folder, making it easy to remember how you produced those outputs hours, days, or years after you reduced the data.\n",
    "\n",
    "The following fit should take ~2 minutes to complete for this current setup if `ncpu` was set to `1` (though the exact runtime is stochastic and also depends on the speed of your CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WFTGkp5c9Qf6",
   "metadata": {
    "id": "WFTGkp5c9Qf6"
   },
   "outputs": [],
   "source": [
    "s5_meta = eureka.S5_lightcurve_fitting.s5_fit.fitlc(eventlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cdae5-650d-4717-b275-7ff2c3f5767f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Optimizing fits to spectroscopic lightcurves\n",
    "\n",
    "Now that we know how to find best-fit values and uncertainties from a single \"white\" lightcurve, it is easy to extrapolate those skills to perform fits to \"spectroscopic\" lightcurves to evaluate how parameters like the eclipse depth vary as a function of wavelength (which can tell us about molecules in the planets' atmospheres)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636fc415-16f3-4b50-b1e5-aeec9e38840d",
   "metadata": {},
   "source": [
    "### 2.1 Adjusting the Stage 5 \"Eureka! Control File\" (__ECF__)\n",
    "\n",
    "Thankfully we can reuse the vast majority of our ECF settings above with only a few minor tweaks.\n",
    "\n",
    "To begin, please first copy below the contents of the ECF that you filled out above. Then:\n",
    "\n",
    "1. Change `fit_method` to `[lsq]` to use the [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) algorithm to **minimize** the **negative** log-probability in order to find the best-fit values for each fitted parameter. Running dynesty on each channel takes quite a long time, so we'll instead just do a quick optimization run with `lsq`.\n",
    "2. Set `lsq_method` to `Powell`. This controls which scipy.optimize.minimize optimization method will be used. In general, the Powell algorithm works very well.\n",
    "3. Set `lsq_tol` to `1e-6`. This sets the error tolerance for the scipy.optimize.minimize optimization method, where a smaller value will result in a more precise estimate of the \"best-fit\" value, so long as the optimizer manages to converge. A value of `1e-6` will suffice for most datasets.\n",
    "4. Leave `lsq_maxiter` at its default setting of `None`. This parameter can be used to let the optimzation routine run for longer if you get a message saying that the fit failed because the maximum number of function evaluations has been exceeded. This can sometimes happen when `lsq_tol` is set too small, when you're fitting a complex model with many parameters, or when there are strong degeneracies between two or more parameters.\n",
    "5. Since we're now fitting spectroscopic lightcurves, we'll adjust the `inputdir` to `Stage4` and the `outputdir` to `Stage5`. If you want to choose a specific Stage4 run, you can do so using the syntax `Stage4/S4_YYYY-MM-DD_miri_runN` where YYYY is the year (e.g., `2025`), MM is the month (e.g., `06`), DD is the day (e.g., `09`), and N is the particular run number that you want to use (e.g., `1`).\n",
    "\n",
    "And that's it for adjustments to the ECF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa2644-c98b-4123-8c2e-532757b34dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_ecf_contents = f\"\"\"\n",
    "\n",
    "# Fill this f-string text block with the contents of the S5 ECF you edited above\n",
    "# and then adjust the values as described above.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(f'S5_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s5_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0c5c6-9554-45fb-939c-9ec12b44f824",
   "metadata": {},
   "source": [
    "### 2.2 Adjusting the Stage 5 \"Eureka! Parameter File\" (__EPF__)\n",
    "\n",
    "Thankfully we can reuse the vast majority of our EPF settings above with only a few minor tweaks.\n",
    "\n",
    "To begin, please first copy below the contents of the EPF that you filled out above. Then you just need to set any parameters that cannot vary with wavelength to be `'fixed'`. The relevant parameters that cannot vary with wavelength are the orbital parameters which are `per`, `t0`, `a`, and `inc`; of these, our previous fit only used `t0`.\n",
    "\n",
    "One option is to fix these parameters to the best-fit values obtained from your optimization run on the white light curve. Alternatively, you could also fix them to the published values from [Bonomo et al. 2017](https://exoplanetarchive.ipac.caltech.edu/overview/WASP-43). We'll go with the former, so copy-paste your best-fit value for `t0` from your above fit into the `Value` column and set the `Free?` column to `'fixed'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744aae0-6249-479d-ada3-9dd7680a2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_epf_contents = f\"\"\"\n",
    "\n",
    "# Fill this f-string text block with the contents of the S5 EPF you edited above\n",
    "# and then adjust the values as described above.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('S5_fit_par_template.epf', 'w') as f:\n",
    "    f.write(s5_epf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98833b28-7e57-40de-b4ff-942c150e717e",
   "metadata": {},
   "source": [
    "### 2.3 Running Eureka!'s Stage 5\n",
    "\n",
    "The following cell will run Eureka!'s Stage 5 using the settings you defined above. Note that your ECF and EPF will be copied to your output folder, making it easy to remember how you produced those outputs hours, days, or years after you reduced the data.\n",
    "\n",
    "The code will loop through each wavelength channel, printing information about the fit and showing plots as it goes.\n",
    "\n",
    "The following fits should take a total of ~1 minute to complete for the 11 channels. While waiting for the fits to complete, you could begin setting up the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae200156-3746-494f-97b4-b471a6560fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_meta = eureka.S5_lightcurve_fitting.s5_fit.fitlc(eventlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c13b5-05f6-494c-bcf4-0f5165a29caa",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Stage 6 - emission and transmission spectra\n",
    "\n",
    "Now that we've completed fits to the 11 spectroscopic channels, we want an easy way of seeing how different parameters vary with wavelength. Of particular interest is the transit depth `rp`, the eclipse depth `fp` (also referred to as day-side flux, when the phase is 0.5), as well as the night-side flux `fn` (when the phase is 0). The variations in day-side and night-side as a function of wavelength is called an emission spectrum. The variations in transit depth as a function of wavelength is called a transmission spectrum. Both transmission and emission spectra can be used to determine the presence and abundance of molecules in the atmosphere of an exoplanet. Because we only used the `lsq` optimization algorithm above, our emission spectrum will not have error bars. If you want to know what the spectrum looks like with error bars, you'll have to re-run your spectroscopic fits above using a sampler like `dynesty`.\n",
    "\n",
    "### 3.1 Setting the Stage 6 \"Eureka! Control File\" (ECF)\n",
    "\n",
    "**This determines what will happen during Eureka!'s Stage 6**\n",
    "\n",
    "To begin, please first copy below the contents of the ECF template for MIRI/LRS from the `S6_template.ecf` file in the ECF demos folder on [GitHub](https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST).\n",
    "\n",
    "The most important parameters and their recommended settings are described below, but more context can be found on the [Eureka! documentation website](https://eurekadocs.readthedocs.io/en/latest/ecf.html#stage-6).\n",
    "\n",
    "1. Set `y_params` to `['fp']`. Eureka! is able to plot any of the `'free'` parameters from your fits as a function of wavelength, but the only two we're really interested in here are `fp`.\n",
    "2. Set `y_scalars` to `[1e6]` so that units of ppm are used for a plot that is easier to read.\n",
    "3. You can safely ignore the settings in the \"Scale height parameters\" chunk and \"Model\" chunk as they are not relevant for our purposes.\n",
    "4. Set `topdir` to `{path_to_data_folder_on_your_machine}` which will use the folder you specified above.\n",
    "5. Since we're dealing with spectroscopic lightcurve fits, we'll adjust the `inputdir` to `Stage5` and the `outputdir` to `Stage6`. If you want to choose a specific Stage5 run, you can do so using the syntax `Stage5/S5_YYYY-MM-DD_miri_runN` where YYYY is the year (e.g., `2025`), MM is the month (e.g., `06`), DD is the day (e.g., `09`), and N is the particular run number that you want to use (e.g., `1`).\n",
    "\n",
    "That's all we need to change for our purposes here, and all the other parameters can be left at their default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb254612-feb4-41bf-8567-c093b0f8935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s6_ecf_contents = f\"\"\"\n",
    "\n",
    "# Fill this f-string text block with the contents of the S6 ECF template\n",
    "# from https://github.com/kevin218/Eureka/tree/exoslam2025/demos/JWST\n",
    "# and then adjust the values as described above.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open(f'S6_{eventlabel}.ecf', 'w') as f:\n",
    "    f.write(s6_ecf_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdb29f-a9be-42ef-a9bc-478df102a9b4",
   "metadata": {},
   "source": [
    "### 3.2 Running Eureka!'s Stage 6\n",
    "\n",
    "The following cell will run Eureka!'s Stage 6 using the settings you defined above. Note that your ECF will be copied to your output folder, making it easy to remember how you produced those outputs hours, days, or years after you reduced the data.\n",
    "\n",
    "The following stage should take &#8810;1 minute to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489c5c6-3fe0-4915-bb7b-9767d17db1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s6_meta = eureka.S6_planet_spectra.s6_spectra.plot_spectra(eventlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f385856-069c-47cc-9dfb-b669bc24a8b2",
   "metadata": {},
   "source": [
    "Observation: As you can see from your emission spectrum, the eclipse depth increases with wavelength. This is generally true of thermal emission spectra because the emission spectrum is approximately a ratio of blackbody functions which increases with increasing wavelength. You should also see a decrease in the eclipse depth around 6.5 microns - this feature is indicative of water in the atmosphere of WASP-43b. To get error bars in the y-direction, you'll need to fit your spectroscopic lightcurves using a sampler like dynesty rather than the lsq fitter we used for expedited fitting here.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Ik5C0D5om1ho7Ud4uE1B3DsJ_BAuzw9_",
     "timestamp": 1687459616849
    },
    {
     "file_id": "1j6hEen7PgwGQK5_qUy6pny-mXLXOTQCn",
     "timestamp": 1678942095588
    },
    {
     "file_id": "16fMugSMlz7uMvikBM_mUgZxjOX_SrvaS",
     "timestamp": 1678938467791
    },
    {
     "file_id": "1o2bFtLDyORQnyDXZn5PPXXBiwvBb_4M8",
     "timestamp": 1678917209033
    }
   ],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
